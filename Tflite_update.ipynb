{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tflite_update.ipynb",
      "provenance": [],
      "mount_file_id": "15H_yJkVH0DEXiwAnupJ_4WshsoUVjR67",
      "authorship_tag": "ABX9TyPPfQP8qF7sE//y7734CL64",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shamoo100/Shamoo_Lytics/blob/master/Tflite_update.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh_iiiv__Ini"
      },
      "source": [
        "# process images\n",
        "\n",
        "# train on transfer learning with model weights\n",
        "\n",
        "# test model\n",
        "\n",
        "# save  model\n",
        "\n",
        "# convert model to tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmgi5eMP9xGU"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "fsaDzGRV-f6l",
        "outputId": "70fa600e-e260-436d-ef5a-4e97a07e23df"
      },
      "source": [
        "base_dir = \"/content/drive/MyDrive/Clock-in-by-Drec/work_faces2/train/\"\n",
        "\n",
        "\"\"\"\n",
        "    # Use `ImageDataGenerator` to rescale the images.\n",
        "    # Create the train generator and specify where the train dataset directory, image size, batch size.\n",
        "    # Create the validation generator with similar approach as the train generator with the flow_from_directory() method.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n    # Use `ImageDataGenerator` to rescale the images.\\n    # Create the train generator and specify where the train dataset directory, image size, batch size.\\n    # Create the validation generator with similar approach as the train generator with the flow_from_directory() method.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cN0lCEj-6p_"
      },
      "source": [
        "# We are trying to minimize the resolution of the images without loosing the 'Features'\n",
        "# For facial recognition, this seems to be working fine, you can increase or decrease it\n",
        "IMAGE_SIZE = 224\n",
        "\n",
        "# Depending upon the total number of images you have set the batch size\n",
        "# I have 50 images per person (which still won't give very accurate result)\n",
        "# Hence, I am setting my batch size to 5\n",
        "# So in 10 epochs/iterations batch size of 5 will be processed and trained\n",
        "BATCH_SIZE = 3\n",
        "\n",
        "# We need a data generator which rescales the images\n",
        "# Pre-processes the images like re-scaling and other required operations for the next steps\n",
        "data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2piTMhYQ_SdK",
        "outputId": "0f9d52a4-3d94-4651-af5b-cb28541b241e"
      },
      "source": [
        "# We separate out data set into Training, Validation & Testing. Mostly you will see Training and Validation.\n",
        "# We create generators for that, here we have train and validation generator.\n",
        "# Create a train_generator\n",
        "train_generator = data_generator.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 331 images belonging to 104 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Syl7Yypb_fJq",
        "outputId": "33ffc6a3-54df-4a39-a1f9-11bff68b18f5"
      },
      "source": [
        "# Create a validation generator\n",
        "val_generator = data_generator.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='validation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 11 images belonging to 104 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq4FU7p0_s0B",
        "outputId": "10edc7dc-a37c-485b-80df-de1137998eb3"
      },
      "source": [
        "# Triggering a training generator for all the batches\n",
        "for image_batch, label_batch in train_generator:\n",
        "    break\n",
        "\n",
        "# This will print all classification labels in the console\n",
        "print(train_generator.class_indices)\n",
        "\n",
        "# Creating a file which will contain all names in the format of next lines\n",
        "labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n",
        "\n",
        "# Writing it out to the file which will be named 'labels.txt'\n",
        "with open('labels.txt', 'w') as f:\n",
        "    f.write(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'abu': 0, 'al': 1, 'b1': 2, 'b2': 3, 'biodun': 4, 'blue_man2': 5, 'bola': 6, 'buss': 7, 'dammie': 8, 'ds': 9, 'fa': 10, 'fairer': 11, 'fareeda': 12, 'fhuad': 13, 'gnemi': 14, 'j1': 15, 'j10': 16, 'j11': 17, 'j12': 18, 'j13': 19, 'j14': 20, 'j15': 21, 'j16': 22, 'j17': 23, 'j18': 24, 'j19': 25, 'j2': 26, 'j20': 27, 'j21': 28, 'j22': 29, 'j23': 30, 'j24': 31, 'j25': 32, 'j26': 33, 'j27': 34, 'j28': 35, 'j29': 36, 'j3': 37, 'j30': 38, 'j4': 39, 'j6': 40, 'j7': 41, 'j8': 42, 'j9': 43, 'john': 44, 'k': 45, 'kano': 46, 'la': 47, 'lanre': 48, 'lara': 49, 'lin': 50, 'moses': 51, 'mouth_man': 52, 'mr_hair': 53, 'mrs kay': 54, 'mubarak': 55, 'nehita': 56, 'o1': 57, 'oz1': 58, 'oz2': 59, 'oz3': 60, 'oz4': 61, 'oz5': 62, 'oz6': 63, 'oz7': 64, 'oz8': 65, 'oz9': 66, 'p6': 67, 'pimple_man': 68, 'real_faces': 69, 'ronke': 70, 'rose': 71, 'rose (1)': 72, 'sag': 73, 'sam': 74, 'sam (1)': 75, 'sam2': 76, 'shamoo': 77, 'smala': 78, 'steph': 79, 'sugaba': 80, 'sunday': 81, 'td': 82, 'teni': 83, 'tg': 84, 'tg (1)': 85, 'tl': 86, 'to': 87, 'toyo': 88, 'toyo (1)': 89, 'tp': 90, 'tuna': 91, 'twin1': 92, 'twin2': 93, 'ugo': 94, 'v': 95, 'v1': 96, 'v2': 97, 'v3': 98, 'vacks': 99, 'vicc': 100, 'victoria': 101, 'wife': 102, 'wman': 103}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8HeKqDWALVc"
      },
      "source": [
        "\"\"\"\n",
        "We have to create the base model from the pre-trained CNN\n",
        "Create the base model from the **MobileNet V2** model developed at Google\n",
        "and pre-trained on the ImageNet dataset, a large dataset of 1.4M images and 1000 classes of web images.\n",
        "First, pick which intermediate layer of MobileNet V2 will be used for feature extraction. \n",
        "A common practice is to use the output of the very last layer before the flatten operation,\n",
        "The so-called \"bottleneck layer\". The reasoning here is that the following fully-connected layers \n",
        "will be too specialized to the task the network was trained on, and thus the features learned by\n",
        "These layers won't be very useful for a new task. The bottleneck features, however, retain much generality.\n",
        "Let's instantiate an MobileNet V2 model pre-loaded with weights trained on ImageNet. \n",
        "By specifying the `include_top=False` argument, we load a network that doesn't include the\n",
        "classification layers at the top, which is ideal for feature extraction.\n",
        "\"\"\"\n",
        "\n",
        "# Resolution of images (Width , Height, Array of size 3 to accommodate RGB Colors)\n",
        "IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "\n",
        "# Create the base model from the pre-trained model MobileNet V2 Hidden and output layer is in the model The first\n",
        "# layer does a redundant work of classification of features which is not required to be trained\n",
        "# (This is also called as bottle neck layer)\n",
        "\n",
        "# Hence, creating a model with EXCLUDING the top layer\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "# Feature extraction\n",
        "# You will freeze the convolution base created from the previous step and use that as a feature extractor\n",
        "# Add a classifier on top of it and train the top-level classifier.\n",
        "\n",
        "# We will be tweaking the model with our own classifications\n",
        "# Hence we don't want our tweaks to affect the layers in the 'base_model'\n",
        "# Hence we disable their training\n",
        "base_model.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K2oQ8uWMn9T",
        "outputId": "820f39d8-653a-4cc8-a28c-2935c989eb5e"
      },
      "source": [
        "# Feature extraction\n",
        "# You will freeze the convolution base created from the previous step and use that as a feature extractor\n",
        "# Add a classifier on top of it and train the top-level classifier.\n",
        "\n",
        "# We will be tweaking the model with our own classifications\n",
        "# Hence we don't want our tweaks to affect the layers in the 'base_model'\n",
        "# Hence we disable their training\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add a classification head\n",
        "# We are going to add more classification 'heads' to our model\n",
        "\n",
        "# 1 -   We are giving our base model (Top Layer removed, hidden and output layer UN-TRAINABLE)\n",
        "\n",
        "# 2 -   2D Convolution network (32 nodes, 3 Kernel size, Activation Function)\n",
        "#       [Remember how Convolutions are formed, with role of kernels]\n",
        "#       [ Kernels in Convents are Odd numbered]\n",
        "#       [ Kernels are just a determinant which is multiplied with Image Matrix]\n",
        "#       [They help in enhancement of features]\n",
        "\n",
        "# 3 -   We don't need all nodes, 20% of nodes will be dropped out [probability of each \"bad\" node being dropped is 20%\n",
        "#       Bad here means the nodes which are not contributing much value to the final output\n",
        "\n",
        "# 4 -   Convolution and pooling - 2X2 matrix is taken and a pooling is done\n",
        "#       Pooling is done for data size reduction by taking average\n",
        "\n",
        "# 5 -   All above are transformation layers, this is the main Dense Layer\n",
        "#       Dense layer takes input from all prev nodes and gives input to all next nodes.\n",
        "#       It is very densely connected and hence called the Dense Layer.\n",
        "#       We are using the Activation function of 'Softmax'\n",
        "#       There is another popular Activation function called 'Relu'\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,  # 1\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu'),  # 2\n",
        "    tf.keras.layers.Dropout(0.2),  # 3\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),  # 4\n",
        "    tf.keras.layers.Dense(104, activation='softmax')  # 5\n",
        "])\n",
        "\n",
        "# You must compile the model before training it.  Since there are two classes, use a binary cross-entropy loss.\n",
        "# Since we have added more classification nodes, to our existing model, we need to compile the whole thing\n",
        "# as a single model, hence we will compile the model now\n",
        "\n",
        "# 1 - BP optimizer [Adam/Xavier algorithms help in Optimization]\n",
        "# 2 - Weights are changed depending upon the 'LOSS' ['RMS, 'CROSS-ENTROPY' are some algorithms]\n",
        "# 3 - On basis of which parameter our loss will be calculated? here we are going for accuracy\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),  # 1\n",
        "              loss='categorical_crossentropy',  # 2\n",
        "              metrics=['accuracy'])  # 3\n",
        "\n",
        "# To see the model summary in a tabular structure\n",
        "model.summary()\n",
        "\n",
        "# Printing some statistics\n",
        "print('Number of trainable variables = {}'.format(len(model.trainable_variables)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 5, 5, 32)          368672    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 104)               3432      \n",
            "=================================================================\n",
            "Total params: 2,630,088\n",
            "Trainable params: 372,104\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n",
            "Number of trainable variables = 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "59j1EkVHNEzW",
        "outputId": "1e20f633-f6e6-4e75-ea8d-d5e9ee7ccdf2"
      },
      "source": [
        "# Train the model\n",
        "# We will do it in 10 Iterations\n",
        "epochs = 10\n",
        "\n",
        "# Fitting / Training the model\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = val_generator)\n",
        "\n",
        "# Visualizing the Learning curves [OPTIONAL]\n",
        "#\n",
        "# Let's take a look at the learning curves of the training and validation accuracy/loss\n",
        "# When using the MobileNet V2 base model as a fixed feature extractor.\n",
        "\n",
        "'''\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()), 1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0, 1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n",
        "\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "111/111 [==============================] - 137s 1s/step - loss: 4.7367 - accuracy: 0.0091 - val_loss: 4.5344 - val_accuracy: 0.1818\n",
            "Epoch 2/10\n",
            "111/111 [==============================] - 22s 196ms/step - loss: 4.5234 - accuracy: 0.0393 - val_loss: 4.3869 - val_accuracy: 0.0909\n",
            "Epoch 3/10\n",
            "111/111 [==============================] - 22s 196ms/step - loss: 4.2102 - accuracy: 0.0544 - val_loss: 3.8710 - val_accuracy: 0.2727\n",
            "Epoch 4/10\n",
            "111/111 [==============================] - 22s 198ms/step - loss: 3.8082 - accuracy: 0.1057 - val_loss: 2.7450 - val_accuracy: 0.2727\n",
            "Epoch 5/10\n",
            "111/111 [==============================] - 22s 200ms/step - loss: 3.3432 - accuracy: 0.1813 - val_loss: 2.3231 - val_accuracy: 0.5455\n",
            "Epoch 6/10\n",
            "111/111 [==============================] - 22s 199ms/step - loss: 2.9247 - accuracy: 0.2689 - val_loss: 2.3165 - val_accuracy: 0.5455\n",
            "Epoch 7/10\n",
            "111/111 [==============================] - 22s 195ms/step - loss: 2.4754 - accuracy: 0.3595 - val_loss: 2.0722 - val_accuracy: 0.4545\n",
            "Epoch 8/10\n",
            "111/111 [==============================] - 22s 195ms/step - loss: 1.9741 - accuracy: 0.4804 - val_loss: 1.8188 - val_accuracy: 0.6364\n",
            "Epoch 9/10\n",
            "111/111 [==============================] - 22s 199ms/step - loss: 1.5833 - accuracy: 0.5650 - val_loss: 1.6496 - val_accuracy: 0.5455\n",
            "Epoch 10/10\n",
            "111/111 [==============================] - 22s 198ms/step - loss: 1.2306 - accuracy: 0.6344 - val_loss: 1.5285 - val_accuracy: 0.6364\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nacc = history.history['accuracy']\\nval_acc = history.history['val_accuracy']\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\nplt.figure(figsize=(8, 8))\\nplt.subplot(2, 1, 1)\\nplt.plot(acc, label='Training Accuracy')\\nplt.plot(val_acc, label='Validation Accuracy')\\nplt.legend(loc='lower right')\\nplt.ylabel('Accuracy')\\nplt.ylim([min(plt.ylim()), 1])\\nplt.title('Training and Validation Accuracy')\\nplt.subplot(2, 1, 2)\\nplt.plot(loss, label='Training Loss')\\nplt.plot(val_loss, label='Validation Loss')\\nplt.legend(loc='upper right')\\nplt.ylabel('Cross Entropy')\\nplt.ylim([0, 1.0])\\nplt.title('Training and Validation Loss')\\nplt.xlabel('epoch')\\nplt.show()\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz-dRs9IL4RE",
        "outputId": "b69c623a-e6fd-4616-f866-a9e65ba5d634"
      },
      "source": [
        "\"\"\"## Fine tuning\n",
        "In our feature extraction experiment, you were only training a few layers on top of an MobileNet V2 base model. The weights of the pre-trained network were **not** updated during training.\n",
        "One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset.\n",
        "### Un-freeze the top layers of the model\n",
        "All you need to do is unfreeze the `base_model` and set the bottom layers be un-trainable. Then, recompile the model (necessary for these changes to take effect), and resume training.\n",
        "\"\"\"\n",
        "\n",
        "# Setting back to trainable\n",
        "base_model.trainable = True\n",
        "\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine tune from this layer onwards\n",
        "fine_tune_at = 100\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of layers in the base model:  154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1gErFGkcyLV",
        "outputId": "91f7f32b-8b5f-483d-f61b-9d72bc888ed8"
      },
      "source": [
        "# Compile the model\n",
        "# Compile the model using a much lower training rate.\n",
        "# Notice the parameter in Adam() function, parameter passed to Adam is the learning rate\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Getting the summary of the final model\n",
        "model.summary()\n",
        "# Printing Training Variables\n",
        "print('Number of trainable variables = {}'.format(len(model.trainable_variables)))\n",
        "\n",
        "# Continue Train the model\n",
        "history_fine = model.fit(train_generator,\n",
        "                         epochs=5,\n",
        "                         validation_data=val_generator\n",
        "                         )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 5, 5, 32)          368672    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 104)               3432      \n",
            "=================================================================\n",
            "Total params: 2,630,088\n",
            "Trainable params: 2,233,544\n",
            "Non-trainable params: 396,544\n",
            "_________________________________________________________________\n",
            "Number of trainable variables = 58\n",
            "Epoch 1/5\n",
            "111/111 [==============================] - 38s 306ms/step - loss: 10.0271 - accuracy: 0.0272 - val_loss: 1.7125 - val_accuracy: 0.5455\n",
            "Epoch 2/5\n",
            "111/111 [==============================] - 33s 294ms/step - loss: 4.3656 - accuracy: 0.1299 - val_loss: 2.1105 - val_accuracy: 0.3636\n",
            "Epoch 3/5\n",
            "111/111 [==============================] - 33s 299ms/step - loss: 3.3390 - accuracy: 0.1873 - val_loss: 2.0573 - val_accuracy: 0.4545\n",
            "Epoch 4/5\n",
            "111/111 [==============================] - 33s 293ms/step - loss: 3.1197 - accuracy: 0.2175 - val_loss: 1.9351 - val_accuracy: 0.5455\n",
            "Epoch 5/5\n",
            "111/111 [==============================] - 33s 292ms/step - loss: 2.8981 - accuracy: 0.2296 - val_loss: 1.7768 - val_accuracy: 0.6364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "iNOBUZsPeOdI",
        "outputId": "6ddc1978-1361-4411-be25-d0133e2a81ae"
      },
      "source": [
        "# Saving the Trained Model to the keras h5 format.\n",
        "# So in future, if we want to convert again, we don't have to go through the whole process again\n",
        "saved_model_dir = 'save/fine_tuning.h5'\n",
        "model.save(saved_model_dir)\n",
        "print(\"Model Saved to save/fine_tuning.h5\")\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "#tf.saved_model.save(model, saved_model_dir)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model_file(model)\n",
        "//Use this if 238 fails tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "# Let's take a look at the learning curves of the training and validation accuracy/loss\n",
        "# When fine tuning the last few layers of the MobileNet V2 base model and training the classifier on top of it.\n",
        "# The validation loss is much higher than the training loss, so you may get some overfitting.\n",
        "# You may also get some overfitting as the new training set is\n",
        "# Relatively small and similar to the original MobileNet V2 datasets.\n",
        "'''\n",
        "'''\n",
        "acc = history_fine.history['accuracy']\n",
        "val_acc = history_fine.history['val_accuracy']\n",
        "loss = history_fine.history['loss']\n",
        "val_loss = history_fine.history['val_loss']\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()), 1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0, 1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "# Summary:\n",
        "'''\n",
        "* **Using a pre-trained model for feature extraction**:  When working with a small dataset, it is common to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier get updated during training.\n",
        "In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.\n",
        "* **Fine-tuning a pre-trained model**: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning.\n",
        "In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the orginial dataset that the pre-trained model was trained on.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Saved to save/fine_tuning.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n* **Using a pre-trained model for feature extraction**:  When working with a small dataset, it is common to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier get updated during training.\\nIn this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.\\n* **Fine-tuning a pre-trained model**: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning.\\nIn this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the orginial dataset that the pre-trained model was trained on.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycy76-MwfBm6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "uzcEs47ffCGF",
        "outputId": "5eb2aff1-0db2-49c4-85d5-17308971ec6e"
      },
      "source": [
        "\n",
        "#tf.saved_model.save(model, saved_model_dir)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "#//Use this if 238 fails tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "# Let's take a look at the learning curves of the training and validation accuracy/loss\n",
        "# When fine tuning the last few layers of the MobileNet V2 base model and training the classifier on top of it.\n",
        "# The validation loss is much higher than the training loss, so you may get some overfitting.\n",
        "# You may also get some overfitting as the new training set is\n",
        "# Relatively small and similar to the original MobileNet V2 datasets.\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "acc = history_fine.history['accuracy']\n",
        "val_acc = history_fine.history['val_accuracy']\n",
        "loss = history_fine.history['loss']\n",
        "val_loss = history_fine.history['val_loss']\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()), 1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0, 1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "# Summary:\n",
        "'''\n",
        "* **Using a pre-trained model for feature extraction**:  When working with a small dataset, it is common to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier get updated during training.\n",
        "In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.\n",
        "* **Fine-tuning a pre-trained model**: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning.\n",
        "In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the orginial dataset that the pre-trained model was trained on.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenetv2_1.00_224_input with unsupported characters which will be renamed to mobilenetv2_1_00_224_input in the SavedModel.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpl5dabhlb/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpl5dabhlb/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n* **Using a pre-trained model for feature extraction**:  When working with a small dataset, it is common to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier get updated during training.\\nIn this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.\\n* **Fine-tuning a pre-trained model**: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning.\\nIn this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the orginial dataset that the pre-trained model was trained on.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}